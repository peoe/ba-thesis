\chapter*{Conclusion}
\phantomsection{}
\addcontentsline{toc}{chapter}{Conclusion}

This thesis addresses the expansion of optimal transport to a large-scale setting via mapping estimation. This is acheived by expanding upon the Sinkhorn algorithm for regularized optimal transport using a stochastic gradient method for learning the desired optimal maps through neural networks.

An introduction to optimal transport is given in the first part. It states the classical Monge formulation of optimal transport, highlights the main disadvantages of this approach, and makes statements about the existence of solutions for atomless starting measures. In order to overcome these issues, the Monge formulation is then relaxed to the Kantorovich problem. It allows for the existence of solutions for more general input measures, as well as the application of duality theory by making use of $c$- and $\bar{c}$-transforms.

In the second part, entropic regularization is introduced with the ide of numerically solving optimal transport problems. The barycentric projection is then used to highlight a method of recovering optimal transport maps from regularized transport plans. Furthermore, a dual formulation of the regularized optimal transport problem allows for the derivation of the Sinkhorn algorithm as a starting point in optimal mapping estimation. Finally, the methods proposed by\ \cite{Seg2018} are used to estimate an optimal transport plan, as well as a corresponding optimal transport map, by modelling them as neural networks and applying stochastic gradient descent algorithms to train these networks.

From here, the estimated plans and maps can be used in a manifold of applications. An approach to apply optimal transport in generative adversarial networks is highlighted in\ \cite{Sal2018}. Image swapping under large initial discrepencies has been investigated by\ \cite{Zhu2020}. Optimal transport has found use in image segmentation, e.g.\ \cite{Rabin2015}. An overview over optimal transport in general image processing is given by\ \cite{Papa2015}. Further foundations for application in computer science can be seen in\ \cite{Levy2017}. More algorithmic and heuristic approaches can be found in~\cite{Pey2019}; they also used optimal transport to derive a solution to auction processes.\ \cite{San2015} has nicely incorporated economic, logistical, probabilistic, statistical and numerical interpretations into their work, by adding discussions at the end of each section.

Connections between optimal transport and more theoretical topcis can also be made. Partial differential equations and gradient flows in particular lend themselves to this, as seen in\ \cite{Ambr2005} and\ \cite{San2015} once more. \textit{Wasserstein distances} and \textit{Wasserstein spaces} play a predominant role in this.

Going forward, mapping estimation in combination with optimal transport can be applied in contexts where complexity costs of the Sinkhorn algorithm have been limiting so far. In addition, direct convergence results may be desirable, as\ \cite{Seg2018} have only performed numerical experiments so far.

%In this thesis we have seen how estimations of large-scale optimal transport plans and maps can be obtained. The latter can easily be employed in applications of optimal transport. When considering optimal transport problems only on a small scale, the Sinkhorn iteration can instead be improved to near-linear time, as shown by~\cite{Alts2019}.

%Several areas of application are given in~\cite{San2015}. In Section~1.7.1 and\ \cite{Seg2018} the general OT problems are seen through a probabilistic lens. Integrals are interpreted as expectations, the starting measures $\mu$ and $\nu$ are used to describe distributions of random variables, and the obtained plan or map are viewed as a joint distribution or transformation between both variables respectively. When considering application in statistical mathematics, Section~2.5.1 suggests an approach to histogram equalization. Off of the motivation of the original Monge Problem, an interpretation in logistical or economical terms is articulated in Section~1.7.3. The logistical point of view is further developed in Section~4.4.1 including traffic equilibria and numerical solutions to continuous transportation problems. An extension to branched transport on graphs and relaxed continuous models is made in Section~4.4.2. Combining the previous two ideas suggests applications in stochastic finance. Section~1.7.5 in particular shows how to extend into martingale optimal transport problems, which may be applied in the theory of hedging financial assets. An approach to apply optimal transport in generative adversarial networks is shown in~\cite{Sal2018}. Improvements in the fairness of statistical learning by using optimal transport have been shown by~\cite{Barr2018}. Further foundations for application in computer science can be found in~\cite{Levy2017}. More algorithmic and heuristic approaches can be found in~\cite{Pey2019}, a solution to auctions via optimal transport in particular may be found in Section~3.7. Optimal transport also finds use in image processing, an overview can be found in~\cite{Papa2015}; particular applications like identity swapping in images with large discrepencies and image segmentation can be found in~\cite{Zhu2020} and~\cite{Rabin2015} respectively.

%Optimal transport may also be used in more theoretical contexts. Using the \textit{Wasserstein distances} between measures allows for the conjunction of optimal transport and Sobolev spaces, as outlined in~\cite{San2015}, Section~5.5.2 and 8.4. This allows for an extension to gradient flows and partial differential equations in general. In Chapter~5, Wasserstein distances are combined with a subset of the space of probability measures to obtain \textit{Wasserstein spaces}. The topology induced by the Wasserstein distance on these spaces are then used to gain insight into constant-speed geodesics on Wasserstein spaces. A more detailed explanation on gradient flows including sections on optimal transport can be found in~\cite{Ambr2005}.