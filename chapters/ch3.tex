\chapter{Optimal Transport}\label{OT}

This section will be dealing with the problem of optimal transport and some results regarding the existence and uniqueness of such transports. There are two important paradigms in the theory of optimal transport. The original formulation is the so called Monge Problem. It was later relaxed by Kantorovich to a more general problem which is easier to handle.

This introductory part will follow~\cite{San2015} Chapter 1 closely in content as well as notation.

\section{The Monge Problem}\label{TheMonProb}
To motivate the formulation of the Monge problem, we consider a mass modelled by a measure on $X$ to be moved to another mass, again modelled by a measure but on $Y$. One might for example imagine a pile of sand or a certain amount of particles being moved from one location to a different one. This transport will incur costs and the goal of optimal transport is to find a ``way of transporting'' the mass to its target whilst accumulating the smallest cost possible, while making sure that no mass is lost.

For this purpose we consider a non-negative \textit{cost function} $c(x, y)$, which will give us the cost of moving a particle from position $x$ to position $y$. Ideally this cost function will be continuous or semi-continuous, though later on we will see that with more constraints we can obtain optimality and uniqueness in a simpler fashion.

To attain the Monge formulation, the transport is modelled via the \textit{push-forward measure}, or \textit{image measure} of our original measure. We will later see, that a more general approach to this transport is beneficial in solving for an optimal solution. For now however, we will constrain ourselves to the earlier approach.

\begin{definition}[Push-Forward; adapted from~\cite{Bog2007}, Section 9.1]\label{PushForward}
	Given a measure $\mu \in \M{X}$ and a measurable map \map[T]{X}{Y}, we define the \textbf{push-forward measure} of $\mu$ under $T$ as a measure on $Y$ by
	\[ (\push{\mu})(y) := \mu(T^{-1} (y)). \]
\end{definition}

A very useful consequence of the push-forward is, that we can use it to state the conservation of mass throughout the transport. Imagine we have a starting measure $\mu$ on $X$ and a target measure $\nu$ on $Y$. In order to not lose any mass, it is necessary for
\[ \push{\mu} = \nu \]
to hold. Formally, this means that all elements of the underlying $\sigma$-algebra $\mathcal{Y}$ on $Y$ coincide under the measures $\nu$ and \push{\mu}, i.e.
\[ \forall A \in \mathcal{Y}: (\push{\mu})(A) = \nu(A).\]

Another useful step to take is to norm the overall mass. This can be acheived by considering only probability measures on $X$ and $Y$.

We can now formulate the Monge Problem in terms of a cost function and the push-forward. This is a modern formulation of the problem originally described by Gaspard Monge in 1781~\cite{Mon1781}.

\todoinline{Should $c$ be restricted to measurable/continuous for clarity?}
\begin{definition}[Monge Problem; adapted from~\cite{San2015}, Problem 1.1]\label{MongeProb}
	Let $\mu \in \PM{X}, \nu \in \PM{Y}$ be our starting and target measures, and \map[c]{X\times{}Y}{\RZero{}\cup{}\{\infty\}}{} be a cost function. Then we define the \textbf{Monge Problem} as
	\begin{equation}\label{MP}
		\inf \left\{ M(T) := \int\limits_X c(x, T(x))~\Dx{\mu} : T \in \MF{X}{Y}, \push{\mu} = \nu \right\}.
	\end{equation}
	From here on, we will be using the abbreviation (MP) as the name of the Monge Problem, and $\inf \text{(MP)}$ will denote the attained infimum.
\end{definition}

\begin{definition}[Optimal Transport Map; adapted from~\cite{San2015}, Section 1.1]\label{OTM}
	A function $T~\in~\MF{X, Y}$ is called an \textbf{optimal transport map}, if it satisfies
	\[ \int\limits_X c(x, T(x))~\Dx{\mu} = \inf \text{(MP)}. \]
\end{definition}

This problem in general is not easy to solve. In order to point out the difiiculites we may face, we require the notions of the \textit{dual space}, \textit{weak convergence} and the \textit{support} of a function.

\begin{definition}[Dual Space; adapted from~\cite{Ryn2008}, Definition 4.28]\label{DuaSpa}
	Let $(X, ||\cdot||)$ be a normed space over \K. We define the \textbf{dual space} of X as the set of all continuous linear functions \map{X}{\K}. Bestowed with the norm $||\varphi||_{X'} := \sup \{ |\varphi(x)| : ||x|| \le 1 \}$, the dual space is a normed space as well.
\end{definition}

\begin{definition}[Weak Convergence for Dual Spaces; adapted from~\cite{Ryn2008}, Definition 5.68]\label{WeakCon}
	Let $(X, ||\cdot||)$ be a normed space, and $(X', ||\cdot||_{X'})$ its dual space. A sequence ${(x_n)}_{\NinN}$ is said to \textbf{converge weakly} to $x \in X$, if
	\[ \forall \varphi \in X': \varphi(x_n) \xrightarrow[\Ninf]{} \varphi(x) \]
	holds. In this case, we write \weak[]{x_n}{x} for \Ninf. The convergence of ${(\varphi(x_n))}_{\NinN}$ is to be understood in the norm $||\cdot||_{X'}$.
\end{definition}

\begin{definition}[Support; adapted from~\cite{San2015}, Definition 1.14]\label{Supp}
	Let $\mu \in \M{X}$ be a measure on a separable metric space $X$. We define the \textbf{support} of $\mu$ as
	\[ \supp{\mu} := \bigcap \{ A \subseteq X : A \text{ closed and } \mu(X \setminus A) = 0 \}. \]
\end{definition}

We can now consider the following two points to illustrate the hardships in dealing with (MP):

Firstly, if one of the two measures were to be discrete, for example of the form $\mu = \delta_{x_0}, x_0 \in X$, and the other is not, (MP) would be infeasible. In order for $T$ to be a transport map in the formulation notation no \textit{mass splitting} mass splitting must occur. With a semidiscrete transport problem, this is simply not possible.
\todoinline{Add notes on why mass splitting must occur?}

Secondly, as~\cite{San2015} pointed out, the constraint on $T$ in (MP) is not closed under weak convergence. For this we consider the sequence $T_n(x) := \sin(nx)$ on $[0, 2 \pi]$ for \NinN. As
\[ \int_0^{2 \pi} \sin^2(nx)~\D[x] = \frac{4 \pi n - \sin(4\pi n)}{4n} = \pi < \infty, \]
we get ${(T_n)}_{\NinN} \in \ELL{2}[0, 2 \pi]$. Combining $(\ELL{2}[0, 2 \pi])' = \ELL{2}[0, 2 \pi]$, $\forall p \in (1, \infty) : \ELL{p}[0, 2\pi] \subseteq \ELL{1}[0, 2\pi]$ and the Riesz-Fr\'{e}chet theorem~(\ref{Rie-Fre}) results in
\[ \forall \varphi \in \ELL{2}[0, 2\pi]~\exists f \in \ELL{2}[0, 2\pi]: \varphi(T_n) = \inner{f}{T_n} = \int\limits_0^{2\pi} f(x) \sin(nx)~\D[x]. \]
We now employ the Riemann-Lebesgue theorem~(\ref{Rie-Leb}) to see that
\[ \lim\limits_{\Ninf} \varphi(T_n) = \lim\limits_{\Ninf} \inner{f}{T_n} = \lim\limits_{\Ninf} \int\limits_0^{2\pi} f(x) \sin(nx)~\D[x] = 0, \]
i.e. $T_n$ converges weakly to $0$ as \Ninf.

With this result, we can now conclude that $T^2 := \lim T_n^2$ as well as $T := \sqrt{T^2}$ are almost surely $0$ with respect to the Lebesgue measure, as $\lim \inner{T_n}{T_n} = 0$ for \Ninf. Hence our target measure may only be discrete, as otherwise $\leb{\supp{T}} \neq 0 \Leftrightarrow T \neq 0~\lambda\text{-a.s.} \Leftrightarrow T^2 \neq 0~\lambda\text{-a.s.}$. As such, our starting measure may also only be discrete, which is a contradiction to the use of the underlying Lebesgue measure in $\ELL{2}[0, 2\pi]$.

Despite these problems of the Monge formulation, we can still provide a couple of results about the existence of transport maps --- though they must not be optimal.

\begin{definition}[Atoms; adapted from~\cite{Bog2007}, Volume 1, Definition 1.12.7]\label{Atoms}
	Let $\mu \in \M{X}$ be a measure and \X{} be a $\sigma$-algebra on $X$. A set $A \in \X$ is called an \textbf{atom} of $\mu$ if
	\[ \forall B \in \X, B \subseteq A : \text{either } \mu(B) = 0 \text{ or } \mu(B) = \mu(A). \]
	If $\mu$ has no atoms, we call it \textbf{atomless}.
\end{definition}

\begin{lemma}[Adapted from~\cite{San2015}, Lemma 1.27]\label{1DTransMapExist}
	Let $\mu, \nu \in \PM{\R}$ be two measures with $\mu$ being atomless. Then, there exists a transport map \map[T]{\R}{\R} such that $\push{\mu} = \nu$.
\end{lemma}

\begin{proof}
	\todoinline{TODO:~\cite{San2015} only gives a very short remark\dots}
\end{proof}

\begin{lemma}[Adapted from~\cite{San2015}, Lemma 1.28]\label{BorelExist}
	There exists an injective Borel-measurable map \map[\sigma_d]{\R^d}{\R} with its image $\text{im}(\sigma_d)$ being a Borel-subset of $\R$ and its inverse map being Borel-measurable as well.
\end{lemma}

\begin{proof}
	Cf.~the proof of Lemma 1.28 in~\cite{San2015}.
\end{proof}

\begin{corollary}[Adapted from~\cite{San2015}, Corollary 1.29]\label{NDTransMapExist}
	Let $\mu, \nu \in \PM{\R^d}$ be two measures with $\mu$ being atomless. Then there exists at least a transport map \map[T]{\R^d}{\R^d} such that $\push{\mu} = \nu$.
\end{corollary}

\begin{proof}
	Using $\sigma_d$ from Lemma~\ref{BorelExist} we obtain the measures \push[(\sigma_d)]{\mu} and \push[(\sigma_d)]{\nu} on \R. By Lemma~\ref{1DTransMapExist} there now exists a transport map \map[\tilde{T}]{\R}{\R} which satisfies $\push[\tilde{T}]{(\push[(\sigma_d)]{\mu})} = \push[(\sigma_d)]{\nu}$. Applying the push-forward with the inverse $\sigma_d^{-1}$ we finally obtain
	\[ \push{\mu} := \push[\sigma_d^{-1}]{(\push[\tilde{T}]{(\push[(\sigma_d)]{\mu})})} = \push[\sigma_d^{-1}]{(\push[(\sigma_d)]{\nu})} = \nu. \]
\end{proof}

\section{The Kantorovich Relaxation}\label{KantRelax}
In~\cite{Kan1942} gave a more general formulation for the optimal transport problem. Instead of restricting the two measures via the push-forward, Kantorovich used so called \textit{transport plans} where the original measures appeared as marginal distributions. This, as~\cite{San2015} puts it,  allowed for a description ofhow many particles are moved from $x$ to $y$ instead of specifying the destination of $x$ under $T$.

\begin{definition}[Transport Plans; adapted from~\cite{San2015}, Problem 1.2]\label{TransPlans}
	Given two measures $\mu \in \PM{X}, \nu \in \PM{Y}$ we define the set of all \textbf{transport plans} from $\mu$ to $\nu$ as
	\[ \TP{\mu}{\nu} := \big\{ \gamma \in \PM{X \times Y} : \push[(\pi_X)]{\gamma} = \mu, \push[(\pi_Y)]{\gamma} = \nu \big\}, \]
	where $\pi_X$ and $\pi_Y$ are the canonical projections onto $X$ and $Y$, i.e.
	\[ \pi_X(x, y) := x, \text{ and } \pi_Y(x, y) := y. \]
\end{definition}

Using this terminology we may now describe the relaxed Kantorovich Problem.

\begin{definition}[Kantorovich Problem; adapted from~\cite{San2015}, Problem 1.2]\label{KanProb}
	Let $\mu \in \PM{X}, \nu \in \PM{Y}$ be two measures and $\map[c]{X \times Y}{\RZero \cup \{\infty\}}$ be a cost function. Then we define the \textbf{Kantorovich Problem} as
	\begin{equation}\label{KP}
		\inf \left\{ K(\gamma) := \int\limits_{X \times Y} c(x, y)~\Dx[x, y]{\gamma} : \gamma \in \TP{\mu}{\nu} \right\}.
	\end{equation}
	In analogy to Definition~\ref{OTM} we will be using the abbreviation (KP) to describe the problem itself and denominate the attained infimum as $\inf \text{(KP)}$. We shall further define \textbf{optimal transport plans} as those transport plans $\gamma \in \TP{\mu}{\nu}$ which satisfy
	\[ \int\limits_{X \times Y} c(x, y)~\Dx[x, y]{\gamma} = \inf \text{(KP)}. \]
\end{definition}

One direct benefit of this formulation is, that we are no longer necessarily bound by the disallowed splitting of mass of (MP). We can however recover the (MP) formulation via (KP) if $\gamma = \push[(id, T)]{\mu}$ for $T \in \MF{X}{Y}$ is an optimal transport plan, in which case $T$ would once again be called an optimal transport map.
\todoinline{Possibly fletch out why this works?}

If given two measures $\mu \in \M{X}, \nu \in \M{Y}$ with $\push{\mu} = \nu$ and a transport map $T \in \MF{X}{Y}$, we can always induce a transport plan from $T$ by defining
\begin{equation}\label{IndPlan}
	\gamma_T := \push[(id, T)]{\mu}.
\end{equation}
We are now interested if this already results in a statement which puts (KP) and (MP) in relation. First we define
\[ J(\gamma) := 
	\begin{cases}
		K(\gamma) = M(T), & \gamma = \gamma_T \\
		\infty, & \text{ otherwise}
	\end{cases}. \]
$J$ immediately allows us to consider both (MP) and (KP) on the same set of admissible objects, that is all transport plans induced by a transport map. As Kantorovich replaced $J$ with $K$ (see Definition~\ref{KanProb}) the question to be asked transforms into: does $\inf K = \inf J$ hold?

To answer this question, we will be giving results which show that the set of plans induced by a map is dense in \TP{\mu, \nu} whenever certain conditions are met. This will then allow us to conclude that indeed both infima coincide. For these results we will always demand compactness for a subset $\Omega \subset \R^d$, however~\cite{San2015} points out, that this is just for simplicity and more general statements may be made.

\begin{definition}[Weak Convergence for Measures; adapted from~\cite{Bog2007}, Volume 2, Definition 8.1.1]\label{WeakConMeas}
	Let $X$ be a measurable space and ${(\mu_n)}_{\NinN}$ be a sequence of measures which satisfies $\forall \NinN: \mu_n \in \M{X}$. This sequence is said to \textbf{converge weakly to a measure} $\mu \in \M{X}$, if for all $f \in \MF{X}{\R}$ 
	\[ \lim\limits_{\NinN} \int\limits_X f(x)~\Dx{\mu_n} = \int\limits_X f(x)~\Dx{\mu} \]
	holds. In this case we will write \weak{\mu_n}{\mu} in analogy to Definition~\ref{WeakCon}.
\end{definition}

\begin{lemma}[Taken from~\cite{San2015}, Lemma 1.31]\label{CompWeakConv}
	Consider on a compact metric space $X$, endowed with a probability $\rho \in \PM{X}$, a sequence of partitions $G_n$, each $G_n$ being a family of disjoint subsets $C_{i, n}$ such that \[ \bigcup\limits_{i \in I_n} C_{i, n} = X \] for every \NinN. Suppose that $\max\limits_{i \in I_n} \text{\normalfont\ diam}(C_{i, n})$ tends to $0$ as \Ninf{} and consider a sequence of probability measures $\rho_n$ on $X$ such that, for every \NinN{} and $i \in I_n$, we have $\rho_n(C_{i, n}) = \rho(C_{i, n})$. Then \weak{\rho_n}{\rho}.
	
	Here, the diameter of a set $A \subseteq X$ is to be understood as
	\[ \text{\normalfont\ diam}(A) := \sup \{ d(x, y) : x, y \in A \}. \]
\end{lemma}

\begin{proof}
	Cf.~the proof of Lemma 1.31 in~\cite{San2015}.
\end{proof}

\begin{theorem}[Set of Plans Induced by Maps is Dense; taken from~\cite{San2015}, Theorem 1.32]\label{IndPlansDense}
	Let $\Omega \subset \R^d$ be a compact subset, and $\mu, \nu \in \PM{\Omega}$ be two measures. If $\mu$ is atomless, then the set of all transport plans $\gamma_T$ induced by a transport map $T$ as defined in~\ref{IndPlan} is dense in the set of all plans \TP{\mu}{\nu}.
\end{theorem}

\begin{proof}
	Cf.~the proof of Theorem 1.32 in~\cite{San2015}.
\end{proof}

With the density of the induced transport plans shown, we can now conclude that the two infima of $J$ and $K$ do coincide:

\begin{definition}[Lower Semi-Continuous Functions; adapted from~\cite{Kes2009}, Definition~5.1.2]\label{lsc}
	Let $X$ be an arbitrary topological space. Then a function \map[f]{X}{\R} is called \textbf{lower semi-continuous} (l.s.c.), if the set
	\[ \{ x \in X : f(x) \le \alpha \} \]
	is closed in $X$ for all $\alpha \in \R$. In particular, every continuous function \map[f]{X}{\R} is also l.s.c.
\end{definition}

\begin{theorem}[Infima of J and K are the Same; adapted from~\cite{San2015}, Theorem~1.33]\label{InfCoincide}
	Let $\Omega \subset \R^d$ be a compact subset, $\mu, \nu \in \PM{\Omega}$ be two measures on $\Omega$ with $\mu$ being atomless, and $\map[c]{\Omega \times \Omega}{\RZero \cup \{ \infty \}}$ be a continuous cost function. Then $\inf J = \inf K$ holds, with the infimum being taken over \TP{\mu}{\nu}.
\end{theorem}

\begin{proof}
	This proof was adapted from~\cite{San2015}, Proof of Theorem~1.33 and Memo on relaxations Box~1.10.

	We consider the relaxation $\overline{J}$ of $J$ defined by
	\[ \forall \gamma \in \TP{\mu}{\nu}: \overline{J}(\gamma) := \inf \left\{ \liminf\limits_{\Ninf} J(\gamma) : \weak{\gamma_n}{\gamma} \right\}. \]
	\todoinline{Is weak convergence correct or should it be strong conv?}
	Since $J \ge \overline{J}$ per definition implies $\inf J \ge \inf \overline{J}$ and as $J \ge \inf J$, with $\inf J$ being constant and thus l.s.c., results in $\overline{J} \ge \inf J$, we get that $\inf J = \inf \overline{J}$. As $K$ is also l.s.c.\ with $K \le J$ we obtain $\inf K \le \inf J$.

	To show $\inf K = J$ we need to find for every transport plan $\gamma$ a sequence of transport maps ${(T_n)}_{\NinN}, T_n \in \MF{\R^d}{\R^d}$ with $\push[(T_n)]{\mu} = \nu$, such that $\gamma_{T_n} \rightharpoonup \gamma$ and $J(\gamma_{T_n}) \rightarrow K(\gamma)$ for \Ninf. Actually, as $K$ is continuous and for $\gamma = \gamma_{T_n}$ it holds that $K(\gamma) = J(\gamma)$, it is already sufficient for a sequence ${(T_n)}_{\NinN}$ to exist with \weak{\gamma_{T_n}}{\gamma}.

	Such a sequence exists, as the set of transport plans induced by a transport map is dense in \TP{\mu}{\nu} by Theorem~\ref{NDTransMapExist}. With this minimizing sequence we finally have $\inf K = \inf \overline{J} = \inf J$.
\end{proof}

\section{The Kantorovich Dual}\label{KanDual}

As~\cite{San2015} points out at the beginning of Section~1.2, the problem (KP) is a linear problem with convex constraints. As such, we are going to investigate the \textit{dual problem} to attain further results. One of these results will be regarding \textit{regularized optimal transport}.
\todoinline{why linear? why convex?}

\todoinline{TODO:~Rewrite section intro when section is done}

We will be expressing the dual problem of (KP) in terms of continuous bounded functions. For this it is useful to restate the constraint $\gamma \in \TP{\mu}{\nu}$ in a different manner. Firstly, we notice that for a general positive measure $\gamma \in \M[+]{X \times Y}$ the equation
\[ \sup\limits_{\varphi \in \CF[b]{X}, \psi \in \CF[b]{Y}} \int\limits_X \varphi(x)~\Dx{\mu} + \int\limits_Y \psi(y)~\Dx[y]{\nu} - \int\limits_{X \times Y} \varphi(x) + \psi(y)~\Dx[x, y]{\gamma} \]
is $0$, if $\gamma \in \TP{\mu}{\nu}$, and $\infty$ otherwise. This effectively constitutes a restatement of our constraint if we add it to the original formulation of (KP): if the constraint is fulfilled, nothing will be changed, and if $\gamma$ is not a transport plan, (KP) will stay infeasible. For ease of notation, we will be using the notation $(\varphi \oplus \psi)(x, y) := \varphi(x) + \psi(y)$. Exchanging the $\inf$ and the $\sup$ in the modified (KP), we obtain
\[ \sup\limits_{\varphi \in \CF[b]{X}, \psi \in \CF[b]{Y}} \int\limits_{X} \varphi~\D + \int\limits_{Y} \psi~\D[\nu] + \inf\limits_{\gamma \in \M[+]{X \times Y}} \int\limits_{X \times Y} c - \varphi \oplus \psi~\D[\gamma] \]
Secondly, we want to restate the above infimum in $\gamma$ as a constraint on $\varphi$ and $\psi$. We get
\[ \inf\limits_{\gamma \in \M[+]{X \times Y}} \int\limits_{X \times Y} c - \varphi \oplus \psi~\D[\gamma] = 
\begin{cases}
	0, & \varphi \oplus \psi \le c \text{ on } X \times Y \\
	- \infty, & \text{else}
\end{cases}. \]
This equation holds, as~\cite{San2015} points out, since if $\varphi \oplus \psi > c$ somewhere in $X \times Y$, we can choose measure $\gamma$ supported on that region with masses tending to $\infty$. 

We can now formulate the dual problem to the Kantorovich Problem:

\begin{definition}[Kantorovich Dual Problem]\label{DualProb}
	Let $\mu \in \PM{X}, \nu \in \PM{Y}$ be our usual measures and $\map[c]{X \times Y}{\RZero{} \cup{} \{ \infty \}}$ be a cost function. Then we define the \textbf{Kantorovich Dual Problem} as
	\begin{equation}\label{DPEq}
		\sup \left\{ \int\limits_{X} \varphi~\D + \int\limits_{Y} \psi~\D[\nu] : \varphi \in \CF[b]{X}, \psi \in \CF[b]{Y}, \varphi \oplus \psi \le c \right\}.
	\end{equation}
	In analogy to Definitions~\ref{MongeProb}~and~\ref{KanProb} we will be using the abbreviation (DP) to describe the problem itself and $\inf \text{(DP)}$ to denominate its infimum.
\end{definition}

With the formulation of (DP) and the previous characterization of the constraint $\gamma \in \TP{\mu}{\nu}$ we can now reach a direct result for the relationship between the primal and the dual poblems. For all admissible $\gamma \in \TP{\mu}{\nu}$ (i.e.~the constraint from (KP)) and $\varphi \in \CF[b]{X}, \psi \in \CF[b]{Y}$ with $\varphi \oplus \psi \le c$ (i.e.~the constraint from (DP)) we have
\[ \sup\limits_{\varphi \in \CF[b]{X}, \psi \in \CF[b]{Y}} \int\limits_{X} \varphi(x)~\Dx{\mu} + \int\limits_{Y} \psi(y)~\Dx[y]{\nu}  \]
\[ = \sup\limits_{\varphi \in \CF[b]{X}, \psi \in \CF[b]{Y}} \int\limits_{X \times Y} (\varphi \oplus \psi)(x, y)~\Dx[x, y]{\gamma} \le \int\limits_{X \times Y} c(x, y)~\Dx[x, y]{\gamma}. \]
As the left hand side of this equality is just a constant, we can further consider the infimum over all $\gamma \in \TP{\mu}{\nu}$ and thus obtain $\sup \text{(DP)} \le \inf \text{(KP)}$.

So far, we do not know if the $\sup$ of the left hand side does exist. To handle this problem, we will be further transforming (DP) using so called \textit{$c$-transforms}, or \textit{$c$-conjugate functions}. With these transformations it will then be possible to formulate (DP) as a optimization problem over just one variable.

\begin{definition}[$c$-Transform, $\bar{c}$-Transform, $c$-Concavity, $\bar{c}$-Concavity; taken from~\cite{San2015}, Definition 1.10]\label{cTrafo}
	Given two functions \map[\chi]{X}{\Rbar} and \map[\zeta]{Y}{\Rbar}, as well as $\map[c]{X \times Y}{\RZero{} \cup{} \{ \infty \}}$ a cost function, we define the \textbf{$\text{\textbf{\textit{c}}}$-transform} of $\chi$ by
	\[ \map[\chi^c]{Y}{\Rbar},~y \mapsto \inf\limits_{x \in X} c(x, y) - \chi(x) \]
	and the \textbf{$\bar{\text{\textbf{\textit{c}}}}$-transform} of $\zeta$ by
	\[ \map[\zeta^{\bar{c}}]{X}{\Rbar},~x \mapsto \inf\limits_{y \in Y} c(x, y) - \zeta(y). \]
	We further define the notion for a function \map[\psi]{Y}{\Rbar} to be \textbf{$\bar{\text{\textbf{\textit{c}}}}$-concave} if there exists a function \map[\chi]{Y}{\Rbar}, such that $\psi = \chi^c$. The set of all $\bar{c}$-concave functions over $Y$ will be denoted by \CBConc{Y}. Analogously, a function \map[\varphi]{X}{\Rbar} is said to be \textbf{$\text{\textbf{\textit{c}}}$-concave} if there exists a function \map[\zeta]{X}{\Rbar}, such that $\varphi = \zeta^{\bar{c}}$, and the set of all such $c$-concave functions over $X$ will accordingly be described by \CConc{X}.
\end{definition}

\todoinline{TODO:~Go via~\cite{San2015} towards convergence of optimal plans --- though possibly a shorter version.~\cite{San2015}, Theorem 1.39 $\rightarrow$ relax to Theorem 1.42 $\rightarrow$ motivate~\cite{Seg2018}, Theorem 1}