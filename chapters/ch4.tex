\chapter{Stochastic Estimation}\label{DualPlanEst}

In the following, $\Omega_1 \subseteq \R^{n_1}$, $\Omega_2 \subseteq \R^{n_2}$ and $\Omega_1 \times \Omega_2 =: \Omega \subseteq \R^{n_1 + n_2}$ will always be compact for some $\NinN[n_1, n_2]$.

We will first introduce regularized optimal transport. This regularization allows for a later duality result, which will then be used to estimate optimal transport plans and maps via methods motivated by the Sinkhorn algorithm.

\section{Regularized Optimal Transport}\label{RegOT}

There are multiple ways that regularization can take place, however we will focus on \textit{entropic regularization}. This form of regularization deals with positive integrable functions with finite entropy, i.e.~for $f \in \MF{\Omega}{\R}$
\[ E(f) := \int\limits_\Omega \vert f(z) \vert \log\left( \vert f(z) \vert \right)~\D[z] < \infty \]
with $0 \cdot \log(0) := 0$. From here we can define
\[ \LLL := \left\{ f \in \MF{\Omega}{\R} : \int\limits_\Omega \vert f(z) \vert \log^+(\vert f(z) \vert)~\D[z] < \infty \right\} \]
as a subset of all integrable functions with finite entropy, where we consider $\log^+(x) := \max \{ 0, \log (x) \}$.

The first conclusion that can be reached via~\cite{Cla2021}, Proposition~2.1, is that $f \in \MF{\Omega}{\RZero}$ is equivalent to $E(f) < \infty$ and $f \in \LLL$.

The second conclusion regards the structure of \LLL{}. When considering the \textit{Luxemburg norm}
\[ \Vert f \Vert_\Phi := \inf \left\{ C > 0 : \int\limits_\Omega \Phi\left( \frac{\vert f(z) \vert}{C} \right)~\D[z] \le 1 \right\} \]
for measurable functions $f$, we denote $\Orlicz{\Phi} := (\mathcal{F}, \Vert \cdot \Vert_\Phi)$, where $\mathcal{F} := \{ f \in \MF{\Omega}{\R} : \Vert f \Vert_\Phi < \infty \}$, the \textit{Orlicz space} for a certain function $\Phi$. Using $\Phi_{\log}(x) := x \log^+ (x)$, we have $\Orlicz{\Phi_{\log}} = \LLL$. In fact, by~\cite{Cla2021}, Theorem~2.5, \LLL{} is a Banach space with respect to its Luxemburg norm.

For a third conclusion we are interested in the dual space of \LLL{}. We first define
\[ \Phi_{\exp}(s) := \begin{cases}
	s, & 0 \le s \le 1 \\
	\exp(s - 1), & s > 1
\end{cases} \]
and then $\Lexp := \Orlicz{\Phi_{\exp}}$ in accordance to its corresponding Luxemburg norm $\Vert \cdot \Vert_{\Phi_{\exp}}$. As shown in~\cite{Cla2021}, Proposition~2.7, if $\Omega$ has finite Lebesgue measure (which it has by $\Omega \subseteq \R^{n_1 + n_2}$ being compact and thus bounded), then ${(\LLL)}' = \Lexp$. Here, ${(\cdot)}'$ denotes the dual space in accordance with\ \cite{Ryn2008}, Definition 4.28

\begin{definition}[Regularized Problem; adapted from~\cite{Cla2021}, (P), (D) and Proposition~3.1]\label{RegProbs}
	We once again consider our starting measures $\mu \in \PM{\Omega_1}$ and $\nu \in \PM{\Omega_2}$, and a cost function $\map[c]{\Omega}{\RZero \cup \{ \infty \}}$. The \textbf{Regularized Problem} for some $\varepsilon > 0$ is defined as
	\[ \inf\limits_{\gamma \in \TP{\mu}{\nu}} \int\limits_\Omega c(x, y)~\Dx[x, y]{\gamma} + \varepsilon \int\limits_\Omega \gamma(x, y) \big( \log(\gamma(x, y)) - 1 \big)~\Dx[x, y]{}. \]
	Its \textbf{Predual Problem} for the same $\varepsilon > 0$ is defined as
	\[ \sup\limits_{\varphi \in \CF[b]{\Omega_1}, \psi \in \CF[b]{\Omega_2}} \int\limits_{\Omega_1} \varphi(x)~\Dx{\mu} + \int\limits_{\Omega_2} \psi(y)~\Dx[y]{\nu} - \varepsilon \int\limits_\Omega F_{\varepsilon}(x, y)~\Dx[x, y]{}, \]
	\[ \text{with } F_{\varepsilon}(x, y) := \exp\left( \frac{1}{\varepsilon} \big( \varphi(x) + \psi(y) - c(x, y) \big) \right). \]
	Just as in the definitions of the previous problems, we will use the abbreviations (RP), $\inf \text{(RP)}$, (PD) and $\sup \text{(PD)}$ for the problems and their solutions respectively. Further, by~\cite{Cla2021}, Proposition~3.1, $\inf \text{(RP)} = \sup \text{(PD)}$, and if $\sup \text{(PD)}$ is finite, (RP) admits a minimizer.
\end{definition}

With Theorem~\ref{KPAdmitPolishLSC} we already saw that an optimal transport plan exists for (KP). For (RP) however, we require more constraints on our marginal measures.

\begin{theorem}[Taken from~\cite{Cla2021}, Theorem~3.3]\label{RegProbAdmitLLL}
	The problem$\text{\normalfont\ (RP)}$ admits a minimizer $\gamma \in \TP{\mu}{\nu}$ if and only if $\mu \in \LLL[\Omega_1]$ and $\nu \in \LLL[\Omega_2]$. In this case, $\gamma \in \LLL$ and $\gamma$ is unique.
\end{theorem}

\begin{proof}
	Cf.~the proof of Theorem~3.3 in~\cite{Cla2021}.
\end{proof}

With a solution to (RP), we can now acheive weak convergence to a solution of (KP). For this we first consider $X$ and $Y$ as complete metric spaces, and $\Omega_1 \subseteq X, \Omega_2 \subseteq Y, \Omega := \Omega_1 \times \Omega_2$ as compact subsets. Given two probability measures $\mu \in \PM{\Omega_1}$ and $\nu \in \PM{\Omega_2}$, we further require two discrete probability measures, weakly converging to $\mu$ and $\nu$ for \Ninf{} respectively, which are defined as
\[ \mu_n := \sum_{i = 1}^n a_i \delta_{x_i}, \nu_n := \sum_{j = 1}^n b_j \delta_{y_j}, \]
with $x_i \in \Omega_1, a_i \ge 0, y_j \in \Omega_2, b_j \ge 0$ for $1 \le i, j \le n$.

\begin{theorem}[Adapted from~\cite{Seg2018}, Theorem~1]\label{RelatRegSolOrigSolPlan}
	Consider $X$, $Y$, $\Omega_1$, $\Omega_2$, $\Omega$, $\mu$, $\mu_n$, $\nu$ and $\nu_n$ as described above. Let \map[c]{\Omega}{\RZero} be a finite and continuous cost function, and ${(\varepsilon_n)}_{\NinN}$ be a sequence converging to $0$ sufficiently fast with $\varepsilon_n > 0$ for all \NinN. Then for ${(\gamma_n^{\varepsilon_n})}_{\NinN}$, the sequence of solutions of$\text{\normalfont\ (RP)}$ between $\mu_n$ and $\nu_n$ with $\varepsilon = \varepsilon_n$, there exists a subsequence ${(\gamma_{n_k}^{\varepsilon_{n_k}})}_{\NinN[k]}$ that weakly converges to a solution $\gamma$ of$\text{\normalfont\ (KP)}$ between $\mu$ and $\nu$ for \Ninf.
\end{theorem}

\begin{proof}
	We consider $\gamma_n$ the solution of (KP) between $\mu_n$ and $\nu_n$ with maximum entropy. According to~\cite{Vill2009}, Theorem~5.20, there now exists a subsequence which weakly converges to a solution $\gamma$ of the same problem between $\mu$ and $\nu$. We continue to label this subsequence as $\gamma_n$, and further take the same indices to form a subsequence from the solutions of (RP) between $\mu_n$ and $\nu_n$, labeling it $\gamma_n^{\varepsilon_n}$. The solutions of (RP) exist due to the discrete measures $\mu_n, \nu_n$ reducing the integral over $\Omega_1$ and $\Omega_2$ respectively to a finite sum over their masses $x_i, y_j$, hence allowing for the application of Theorem~\ref{RegProbAdmitLLL}. By expanding the following integral for $g \in \CF[b]{\Omega}$
	\[ \Bigg\lvert \int\limits_{\Omega} g~\D[\gamma_n^{\varepsilon_n}] - \int\limits_{\Omega} g~\D[\gamma] \Bigg\rvert = \Bigg\lvert \int\limits_{\Omega} g~\D[\gamma_n^{\varepsilon_n}] - \int\limits_{\Omega} g~\D[\gamma_n] \Bigg\rvert + \Bigg\lvert \int\limits_{\Omega} g~\D[\gamma_n] - \int\limits_{\Omega} g~\D[\gamma] \Bigg\rvert, \]
	we now only have to show that the left summand on the right hand side converges to $0$, as the right summand converges by the previously mentioned statement of~\cite{Vill2009}. As $\gamma_n^{\varepsilon_n}$ and $\gamma_n$ are solutions to optimal transport problems between discrete measures, we can replace the integral over $\Omega$ with sums over the masses of both measures and obtain
	\[ \Bigg\lvert \sum\limits_{i, j = 1}^n g(x_i, y_j) \gamma_n^{\varepsilon_n}(x_i, y_j) - \sum\limits_{i, j = 1}^n g(x_i, y_j) \gamma_n(x_i, y_j) \Bigg\rvert \le M_g \Vert \gamma_n^{\varepsilon_n} - \gamma_n \Vert_{1}^{n \times n}, \]
	where $\Vert \gamma_n^{\varepsilon_n} - \gamma_n \Vert_{1}^{n \times n} := \sum\limits_{i, j = 1}^n \big\lvert \gamma_n^{\varepsilon_n}(x_i, y_j) - \gamma_n(x_i, y_j) \big\rvert$ and $M_g := \max g(x_i, y_j)$ over $1 \le i, j \le n$. Adapting a result from~\cite{Comi1994}, specifically Equation~2 in the proof of Proposition~3.1, there exist $M_{c_n, \mu_n, \nu_n}, \lambda_{c_n, \mu_n, \nu_n} > 0$ such that
	\[ \Vert \gamma_n^{\varepsilon_n} - \gamma_n \Vert_1^{n \times n} \le M_{c_n, \mu_n, \nu_n} \exp\left( \frac{-\lambda_{c_n, \mu_n, \nu_n}}{\varepsilon_n} \right), \]
	with $c_n := {\big( c(x_i, y_j) \big)}_{i, j = 1}^n$. To show the convergence of the right hand side we can finally choose $\varepsilon_n = \lambda_{c_n, \mu_n, \nu_n} \cdot {\ln\left( n\,M_{c_n, \mu_n, \nu_n} \right)}^{-1} \rightarrow 0, \Ninf$.
\end{proof}

\section{The Barycentric Projection}\label{BaryProj}

In the first part, specifically in Theorem~\ref{IndPlansDense}, we already saw that the set of transport plans induced by transport maps is dense in the set of all transport maps \TP{\mu}{\nu}. We are now interested in finding a transport map from our regularized optimal transport plan, i.e.~a solution to (RP) between $\mu$ and $\nu$, since the handling of a mapping instead of a joint measure (probability distribution) is often advantageous in direct applications. As we have already seen in Theorem~\ref{InfCoincide}, when $\mu$ is atomless, such a transport map is directly related to the accompanying transport plan. This suggests that we start with our weakly convergent subsequence and apply a transformation which will then result in a sequence weakly converging toward an optimal transport map. The transformation chosen by\ \cite{Seg2018}, is the so called \textit{barycentric projection}.

\begin{definition}[Barycentric Projection; adapted from~\cite{Seg2018}, Definition~1]\label{BarCentrProj}
	We consider a transport plan $\gamma \in \TP{\mu}{\nu}$ and a convex cost function $\map[d]{\Omega_2 \times \Omega_2}{\RZero \cup \{ \infty \}}$. Then the \textbf{barycentric projection} of $\gamma$ at $x \in \Omega_1$ is defined as
	\[ \bar{\gamma}(x) := \argmin\limits_{z \in \Omega_2} \int\limits_{\Omega_2} d(z, y)~\Dx[x, y]{\gamma}. \]
	If $d = \Vert x - y \Vert_2^2$, where $\Vert \cdot \Vert_2$ is the Euclidean norm, the barycentric projection has the closed form
	\[ \bar{\gamma}(x) = \int\limits_{\Omega_2} y~\Dx[x, y]{\gamma}. \]
\end{definition}

If $c(x, y) = \Vert x - y \Vert_2^2$, according to~\cite{Seg2018}, it can be further shown, that the barycentric projection of an optimal transport plan for (KP) is already an optimal transport map for (MP).

Similarly to Theorem~\ref{RelatRegSolOrigSolPlan}, we will consider $X = Y = \R^d$, $\Omega_1, \Omega_2 \subseteq \R^d, \Omega := \Omega_1 \times \Omega_2$ compact, a continuous measure $\mu \in \PM{\Omega_1}$ satisfying~\cite{Vill2009}, Corollary~9.3, an arbitrary measure $\nu \in \PM{\Omega_2}$, a continuous cost function $\map[c]{\R^d \times \R^d}{\RZero \cup \{ \infty \}}$, as well as the discrete measures
\[ \mu_n := \frac{1}{n} \sum\limits_{i = 1}^n a_i \delta_{x_i}, \nu_n := \frac{1}{n} \sum\limits_{j = 1}^n b_j \delta_{y_j}, \]
with $x_i \in \Omega_1, a_i \ge 0, y_j \in \Omega_2, b_j \ge 0$ for $1 \le i, j \le n$, weakly converging to $\mu$ and $\nu$ respectively.

\begin{theorem}[Adapted from~\cite{Seg2018}, Theorem~2]\label{RelatRegSolOrigSolMap}
	Consider $X$, $Y$, $\Omega_1$, $\Omega_2$, $\Omega$, $\mu$, $\nu$, $c$, $\mu_n$, $\nu_n$ as described above. Assume that the solutions $\gamma_n$ to$\text{\normalfont\ (KP)}$ between $\mu_n$ and $\nu_n$ are unique for all \NinN. Further let ${(\varepsilon_n)}_{\NinN}$ be a sequence converging sufficiently fast to $0$ with $\varepsilon_n > 0$ for all \NinN, and $d$ be a convex cost function on $\Omega_2 \times \Omega_2$. Then there exists a subsequence ${\big(\bar{\gamma}^{\varepsilon_{n_k}}_{n_k}\big)}_{\NinN[k]}$ such that \weak{\push[\big(id_{\Omega_1}, \bar{\gamma}^{\varepsilon_{n_k}}_{n_k}\big)]{\mu_n}}{\push[(id_{\Omega_1}, T)]{\mu}} for \Ninf, where $T$ is the map solving$\text{\normalfont\ (MP)}$ between $\mu$ and $\nu$, $id_{\Omega_1}$ is the identity map on $\Omega_1$, and $\bar{\gamma}^{\varepsilon_{n_k}}_{n_k}$ is the barycentric projection of the solution $\gamma^{\varepsilon_{n_k}}_{n_k}$ of$\text{\normalfont\ (RP)}$ with respect to $d$.
\end{theorem}

\begin{proof}
	We know that $\push[\bar{\gamma}_n^{\varepsilon_n}]{\mu_n} - \push{\mu} = \push[\bar{\gamma}_n^{\varepsilon_n}]{\mu_n} - \push[\bar{\gamma}_n]{\mu_n} + \push[\bar{\gamma}_n]{\mu_n} - \push{\mu}$. When considering the absolute value of the left hand side and applying the triangle inequality to the right hand side, we can prove the weak convergence claimed in the theorem by proving it for both summands on the right hand side.	By~\cite{Vill2009}, Corollary~9.3, there exists a Monge map between $\mu$ and $\nu$ due to $\mu$ being uniformly continuous on $\Omega_1$ and thus absolutely continuous with respect to the Lebesgue measure. Following the proof of~\cite{Seg2018}, Theorem~2, we see that the summand involving $\push[\bar{\gamma}_n]{\mu_n} - \push[T]{\mu}$ converges.
	
	We thus only have to show that for any Lipschitz continuous function $g$ over $\Omega$
	\[ \Bigg\lvert \int\limits_{\Omega} g~\D[{\push[(id_{\Omega_1}, \bar{\gamma}_n^{\varepsilon_n})]{\mu_n}}] - \int\limits_{\Omega} g~\D[{\push[(id_{\Omega_1}, \bar{\gamma}_n)]{\mu_n}}] \Bigg\rvert \rightarrow 0 \]
	for \Ninf{} and $\varepsilon_n$ converging to $0$ sufficiently fast, as \weak{\gamma_n}{T} by~\cite{Vill2009}, Theorem~5.20. According to~\cite{Seg2018}, the optimal transport plan $\gamma_n$ is induced by an optimal map $T_n$, i.e.~$\gamma_n = \push[(id_{\Omega_1}, T_n)]{\mu_n}$, which then further implies $\push[(id_{\Omega_1}, \bar{\gamma}_n)]{\mu_n} = \push[(id_{\Omega_1}, T_n)]{\mu_n}$. This allows us to apply the definition of the push forward and rewrite
	\[ \int\limits_{\Omega} g~\D[{\push[(id_{\Omega_1}, \bar{\gamma}_n^{\varepsilon_n})]{\mu_n}}] = \int\limits_{\Omega} g\big( x, \bar{\gamma}_n^{\varepsilon_n}(x) \big)~\Dx{\mu_n} = \frac{1}{n} \sum\limits_{i = 1}^n g\big( x_i, \bar{\gamma}_n^{\varepsilon_n}(x_i) \big), \]
	\[ \int\limits_{\Omega} g~\D[{\push[(id_{\Omega_1}, \bar{\gamma}_n)]{\mu_n}}] = \int\limits_{\Omega} g\big( x, T_n(x) \big)~\Dx{\mu_n} = \frac{1}{n} \sum\limits_{i = 1}^n g\big( x_i, T_n(x_i) \big). \]
	We apply the closed form of the barycentric projection as given by~\cite{Seg2018} $\bar{\gamma}(x) = \gamma(x) Y$, with $Y := {(y_1, \dots, y_n)}^T$ and $\gamma(x) := (\gamma(x, y_1), \dots \gamma(x, y_n))$, to obtain
	\[ \Bigg\lvert \int\limits_{\Omega} g\big( x, \bar{\gamma}_n^{\varepsilon_n}(x) \big)~\Dx{\mu_n} - \int\limits_{\Omega} g\big( x, T_n(x) \big)~\Dx{\mu_n} \Bigg\rvert \le \sum\limits_{i = 1}^n K_g \big\vert \bar{\gamma}_n^{\varepsilon_n}(x_i) - \bar{\gamma}_n(x_i) \big\vert \]
	\[ = n~K_g~\Vert \gamma_n^{\varepsilon_n} Y - \gamma_n Y \Vert_{\R^{n \times d}, 2} \le n~K_g~\Vert Y \Vert_{\R^{n \times d}, 2}^{1/2}~\Vert \gamma_n^{\varepsilon_n} - \gamma_n \Vert^{1/2}_{\R^{n \times n}, 2}, \]
	where we use $K_g < \infty$ the Lipschitz constant of $g$, $\Vert A \Vert_{\R^{d \times n}, 2} = \sigma_{\max}(A^T A)$, $\Vert A \Vert_{\R^{n \times n}, 2} = \sigma_{\max}(A^T A)$, $\gamma_n^{\varepsilon_n} = {\big( \gamma_n^{\varepsilon_n}(x_i, y_j) \big)}_{i, j = 1}^n$, $\gamma_n$ in a similar fashion, and Cauchy-Schwarz is used on the last inequality. Using the same result by~\cite{Comi1994} as in the proof of Theorem~\ref{RelatRegSolOrigSolPlan}, we can now find $M_{c_n, \mu_n, \nu_n} > 0$ and $\lambda_{c_n, \mu_n, \nu_n} > 0$ such that
	\[ \Vert \gamma_n^{\varepsilon_n} - \gamma_n \Vert^{1/2}_{\R^{n \times n}, 2} \le M_{c_n, \mu_n, \nu_n} \exp\left( \frac{-\lambda_{c_n, \mu_n, \nu_n}}{\varepsilon_n} \right). \]
	Choosing $\varepsilon = \lambda_{c_n, \mu_n, \nu_n} \cdot {\ln\left( n^2 \Vert Y \Vert^{1/2}_{\R^{n \times d}, 2} M_{c_n, \mu_n, \nu_n} \right)}^{-1}$ suffices for the right hand side to converge to $0$ for any Lipschitz continuous $g$ over $\Omega$, showing \weak{\bar{\gamma}_n^{\varepsilon_n}}{T} for \Ninf.
\end{proof}

\todoinline{TODO:~Consider whether or not to instead label $Y$ as $Y_n$, mimicking~\cite{Seg2018}}

\begin{corollary}[Taken from~\cite{Seg2018}, Corollary~1]\label{RelatRegOrigCor}
	With the same assumptions as for Theorem~\ref{RelatRegSolOrigSolMap}, there exists a subsequence ${(n_k)}_{\NinN[k]}, \NinN[n_k]$ for all \NinN[k], such that \weak{\push[\bar{\gamma}^{\varepsilon_{n_k}}_{n_k}]{\mu_{n_k}}}{\nu} for \Ninf.
\end{corollary}

\begin{proof}
	We consider $h \in \CF[b]{\Omega_2}$ and further define \map[g]{\Omega}{\R} by $g(x, y) := h(y)$ for all $(x, y) \in \Omega$. As $h$ is bounded and continuous, $g$ is as well. With the same notation as in the proof of Theorem~\ref{RelatRegSolOrigSolMap}, we see that
	\[ \Bigg\lvert \int\limits_{\Omega_2} h~\D[{\push[\bar{\gamma}_n^{\varepsilon_n}]{\mu_n}}] - \int\limits_{\Omega_2} h~\D[{\push{\mu}}] \Bigg\rvert = \Bigg\lvert \int\limits_{\Omega} g~\D[{\push[(id_{\Omega_1}, \bar{\gamma}_n^{\varepsilon_n})]{\mu_n}}] - \int\limits_{\Omega} g~\D[{\push[(id_{\Omega_1}, T)]{\mu}}] \Bigg\rvert. \]
	Applying the theorem mentioned above we see that the left hand side converges to $0$ for a subsequence if $\varepsilon_n$ converges sufficiently fast. Finally, we use $\push{\mu} = \nu$ to prove the corollary.
\end{proof}

\section{The Regularized Dual}\label{RegDual}

As we have previously seen, the regularization term is finite if and only if $\gamma \in \LLL$. By considering (RP) over \LLL{} instead, we can also derive a dual problem over $\Lexp[\Omega_1] \times \Lexp[\Omega_2]$. Following the procedure from~\cite{Cla2021}, Section~4, we define
\[ \Phi(s) := \begin{cases}
	\infty, & s < 0 \\
	s, & 0 \le s \le 1 \\
	\exp(s - 1), & s > 1
\end{cases}, \Psi(s) := \begin{cases}
	- \infty, & s \le 0 \\
	\log(s), & 0 < s \le 1 \\
	s - 1, & s > 1
\end{cases}, \]
\[ u_1 := \begin{cases}
	\exp \left( \frac{\varphi}{\varepsilon} \right), & \varphi \le 0 \\
	\frac{\varphi}{\varepsilon} + 1, & \varphi > 0
\end{cases} \text{, and } u_2 := \begin{cases}
	\exp \left( \frac{\psi}{\varepsilon} \right), & \psi \le 0 \\
	\frac{\psi}{\varepsilon}, & \psi > 0
\end{cases} \]
and obtain $\varphi = \varepsilon \log \big( \Phi(u_1) \big) = \varepsilon \Psi(u_1) \text{ and } \psi = \varepsilon \log \big( \Phi(u_2) \big) = \varepsilon \Psi(u_2)$. When put back into (PD), this gives us
\[ \int\limits_{\Omega_1} \varphi(x)~\Dx{\mu} + \int\limits_{\Omega_2} \psi(y)~\Dx[y]{\nu} - \varepsilon \int\limits_{\Omega} \exp \left( \frac{\varphi(x) + \psi(y) - c(x, y)}{\varepsilon} \right)~\Dx[x, y]{} \]
\[ = \varepsilon \int\limits_{\Omega_1} \Psi(u_1(x))~\Dx{\mu} + \varepsilon \int\limits_{\Omega_2} \Psi(u_2(y))~\Dx[y]{\nu} - \varepsilon \int\limits F_{\varepsilon}^{\Phi}(x, y)~\Dx[x, y]{}, \]
with $F_{\varepsilon}^{\Phi}(x, y) := \Phi\big( u_1(x) \big) \Phi\big( u_2(y) \big) \exp\left( \frac{-c(x, y)}{\varepsilon} \right)$.

%We are now interested in how this estimate for the regularized problem (RP) may result in a solution for the original problem (KP). Applying Equation~\ref{RegApprTransPlan}, we denote the approximated transport plan for a certain $\varepsilon > 0$ by $\gamma^\varepsilon$.\@~\cite{Seg2018}, Theorem~1, gives us a result regarding the convergence of regularized optimal transport plans.

We note that $\Phi = \Phi_{\exp}$ on \RZero, suggesting an optimization over ${(u_1, u_2)} \in \Lexp[\Omega_1] \times \Lexp[\Omega_2]$ with $u_1, u_2 \ge 0$ (as the problem would be infeasible otherwise).

\begin{definition}[Regularized Dual; adapted from~\cite{Cla2021}, ($\text{D}_{\exp}$)]\label{RegDualProb}
	Let $\mu, \nu, \varepsilon$ as well as $c$ be the same as in Definition~\ref{RegProbs}. We consider the set $\mathcal{F} := \{ (f, g) \in \Lexp[\Omega_1] \times \Lexp[\Omega_2] : f, g \ge 0 \}$ and define the \textbf{Regularized Dual Problem} as
	\[ \sup\limits_{\mathcal{F}} \left\{ \varepsilon \int\limits_{\Omega_1} \Psi(u_1(x))~\Dx{\mu} + \varepsilon \int\limits_{\Omega_2} \Psi(u_2(y))~\Dx[y]{\nu} - \varepsilon \int\limits_{\Omega} F_{\varepsilon}^{\Phi}(x, y)~\Dx[x, y]{} \right\}. \]
	This problem will accordingly be labelled as (RD) and its supremum as $\sup \text{(RD)}$. The restriction on $\mathcal{F}$ ensures that all integrals are well defined.
\end{definition}

\begin{lemma}[Adapted from~\cite{Cla2021}, Theorem~4.6]\label{RegDualAdmit}
	The problem$\text{\normalfont\ (RD)}$ admits a solution $(\bar{u}_1, \bar{u}_2) \in \Lexp[\Omega_1] \times \Lexp[\Omega_2]$.
\end{lemma}

\begin{proof}
	Cf.~the proof of Theorem~4.6 in~\cite{Cla2021}.
\end{proof}

Using the previously seen, we can now resubstitute $\bar{\phi} = \varepsilon \Psi(\bar{u}_1)$ as well as $\bar{\psi} = \varepsilon \Psi(\bar{u}_2)$ from our dual solutions. We can further assume $\bar{u}_1 > 0$ $\mu$-a.e.\ and $\bar{u}_2 > 0$ $\nu$-a.e., as otherwise $\int \Psi(\bar{u}_1)~\D + \int \Psi(\bar{u}_2)~\D[\nu] = -\infty$, rendering the problem infeasible. The pair $(\bar{\phi}, \bar{\psi})$ however is not guaranteed to be admissible in (PD), i.e.~$\CF[b]{\Omega_1} \times \CF[b]{\Omega_2}$, as even for $\bar{u}_1, \bar{u}_2 > 0$ we do not necessarily get bounds on $\bar{\phi}$ and $\bar{\psi}$, since $\Psi(x) \rightarrow -\infty$ for $x \rightarrow 0$.

If we instead restrict ourselves to the assumptions from Theorem~\ref{RegProbAdmitLLL}, we obtain strong duality between (RP) and (RD).

\begin{theorem}[Adapted from~\cite{Cla2021}, Proposition~4.7]\label{RegStrongDualityLLL}
	We consider $\mu \in \LLL[\Omega_1], \nu \in \LLL[\Omega_2]$ and $c \in \CF{\Omega}$. Then$\text{\normalfont\ (RP)}$ admits a solution $\bar{\gamma} \in \LLL$,$\text{\normalfont\ (RD)}$ admits a solution $(\bar{u}_1, \bar{u}_2) \in \Lexp[\Omega_1] \times \Lexp[\Omega_2]$, and $\sup\!\text{\normalfont\,(RD)} = \inf\!\text{\normalfont\,(RP)}$ holds.
\end{theorem}

\begin{proof}
	Cf.~the proof of Theorem~4.8 in~\cite{Cla2021}.
\end{proof}

\begin{corollary}[Conditions on Dual Optimality; adapted from~\cite{Cla2021}, Theorem~4.8]\label{RegOptCond}
	Let $\mu \in \LLL[\Omega_1], \nu \in \LLL[\Omega_2]$ and $c \in \CF{\Omega}$, as in Theorem~\ref{RegStrongDualityLLL}. Then for $\mu$-a.e.\ $x \in \Omega_1$ and $\nu$-a.e.\ $y \in \Omega_2$ all dual solutions $(\bar{u}_1, \bar{u}_2) \in \Lexp[\Omega_1] \times \Lexp[\Omega_2]$ of$\text{\normalfont\ (RD)}$ satisfy
	\[ \mu(x) = \Phi\big( \bar{u}_1(x) \big) \int\limits_{\Omega_2} \Phi\big( \bar{u}_2(y) \big) \exp \left( \frac{- c(x, y)}{\varepsilon} \right)\D[y], \]
	and
	\[ \nu(y) = \Phi\big( \bar{u}_2(y) \big) \int\limits_{\Omega_1} \Phi\big( \bar{u}_1(x) \big) \exp \left( \frac{- c(x, y)}{\varepsilon} \right)\D[x]. \]
	Furthermore, a solution $\bar{\gamma} \in \LLL$ of$\text{\normalfont\ (RP)}$ is defined by
	\[ \bar{\gamma}(x, y) = \Phi\big( \bar{u}_1(x) \big) \Phi\big( \bar{u}_2(y) \big) \exp\left( \frac{- c(x, y)}{\varepsilon} \right). \]
\end{corollary}

\begin{proof}
	Cf.~the proof of Theorem~4.8 in~\cite{Cla2021}.
\end{proof}

As~\cite{Cla2021}, Remark~4.9, points out, we can use these optimality conditions to derive the \textit{Sinkhorn algorithm}. We make use of the first two equations to iteratively approach optimal dual variables, and apply the last equation to obtain an estimated optimal transport plan. For a starting value $u_2^0 \in \Lexp[\Omega_2]$ we define
\[ T_1^{n + 1}(x) := \Phi^{-1} \left( \frac{\mu(x)}{\int_{\Omega_2} \Phi\big( u_2^n(y) \big) \exp\left( \frac{-c(x, y)}{\varepsilon} \right)\D[y]} \right), \]
and
\[ T_2^{n + 1}(y) := \Phi^{-1} \left( \frac{\nu(y)}{\int_{\Omega_2} \Phi\big( u_1^{n + 1}(x) \big) \exp\left( \frac{-c(x, y)}{\varepsilon} \right)\D[x]} \right) \]
for all \NinN, $(x, y) \in \Omega$.
\begin{algorithm}\label{SinkhornAlg}
	\caption{Sinkhorn Algorithm; adapted from~\cite{Cla2021}, Remark~4.9}
	\KwResult{Optimal dual variables $\bar{u}_1, \bar{u}_2$}
	\KwIn{$\mu \in \PM{\Omega_1}, \nu \in \PM{\Omega_2}, (x, y) \in \Omega, \varepsilon > 0$, cost function $c$, starting variable $u_2^0 \in \Lexp[\Omega_2]$}
	$n := 0$\;
	\While{$\text{\normalfont{}not converged}$}{
		$u_1^{n + 1}(x) := T_1^{n + 1}(x)$\;
		$u_2^{n + 1}(x) := T_2^{n + 1}(y)$\;
		$n := n + 1$\;
	}
\end{algorithm}

Stopping after $N - 1$ iterations, we get
\[ \gamma_N(x, y) = \Phi\big( u_1^N(x) \big) \Phi\big( u_2^N(y) \big) \exp\left( \frac{- c(x, y)}{\varepsilon} \right) \]
as an approximate value of transport plan for (RP) at a predetermined $(x, y)$. Resubstituting once more with $\exp\left( \frac{\varphi_N(x)}{\varepsilon} \right) = \Phi\big( u_1^N(x) \big)$ as well as $\exp\left( \frac{\psi_N(y)}{\varepsilon} \right) = \Phi\big( u_2^N(y) \big)$, we obtain
\begin{equation}\label{RegApprTransPlan}
	\gamma_N(x, y) = \exp\left( \frac{\varphi_N(x) + \psi_N(y) - c(x, y)}{\varepsilon} \right).
\end{equation}

%This formulation will be used from now on, as it simplifies the following formulae, and by the proof of Theorem~\ref{RegStrongDualityLLL} $\sup\limits_{L_{\exp}} \text{(RD)}$ and $\sup\limits_{\mathcal{C}_b} \text{(PD)}$ coincide.

\section{Stochastic Plan and Mapping Estimation}\label{StoPlanAndMapEst}

\todoinline{TODO:~add phrases to incorporate previous theorems, showing that this estimation is actually based on theory!}

As~\cite{Seg2018}, Section~1, Large-scale OT, points out, the Sinkhorn algorithm is only useful when considering discrete measures over small samples as each of its iterations has $\mathcal{O}(n^2)$ complexity. Hence, another approach is required when the underlying measures are continuous or the individual sample sizes get too large.

The procedure taken by~\cite{Seg2018} improves on both shortcomings: Instead of only computing the approximated plan at a specified location $(x, y)$ a neural network is trained to model the continuous case and a finite vector is iteratively improved in the discrete case. To deal with larger samples, the input measures $\mu$ and $\nu$ are interpreted as probability distributions and individual samples drawn from them for each iteration.

In order to state the algorithm, we need to adapt the previous definition of $F_\varepsilon$ slightly to include the dual variables:
\[ F_\varepsilon\big( \varphi(x), \psi(y) \big) := \exp\left( \frac{1}{\varepsilon} \big( \varphi(x) + \psi(y) - c(x, y) \big) \right). \]
When considering the continuous case, we further define $\nabla \varphi$ and $\nabla \psi$ as the gradients with respect to the parameters of the underlying neural networks, and the notions of $\partial_\varphi F_\varepsilon$ and $\partial_\psi F_\varepsilon$ in accordance with these gradients.

\begin{algorithm}\label{OTPlanEstAlg}
	\caption{Transport Plan Estimation; adapted from~\cite{Seg2018}, Algorithm~1}
	\KwResult{Estimates of optimal dual variables $\varphi, \psi$}
	\KwIn{$\mu \in \PM{\Omega_1}, \nu \in \PM{\Omega_2}, \varepsilon > 0$, cost function $c$, batch size $p$, learning rate $\rho$}
	Initialize $\varphi_0, \psi_0$\;
	$n := 0$\;
	\While{$\text{\normalfont{}not converged}$}{
		sample a batch $(x_1, \dots, x_p)$ from $\mu$\;
		sample a batch $(y_1, \dots, y_p)$ from $\nu$\;
		$\varphi_{n + 1} := \varphi_n + \rho \sum\limits_{i, j = 1}^p \Big( \nabla \varphi_n(x_i) - \varepsilon \partial_{\varphi_n} F_\varepsilon\big( \varphi_n(x_i), \psi_n(y_j) \big) \nabla\varphi_n(x_i) \Big)$\;
		$\psi_{n + 1} := \psi_n + \rho \sum\limits_{i, j = 1}^p \Big( \nabla \psi_n(x_i) - \varepsilon \partial_{\psi_n} F_\varepsilon\big( \varphi_n(x_i), \psi_n(y_j) \big) \nabla\psi_n(y_j) \Big)$\;
		$n := n + 1$\;
	}
\end{algorithm}

The most apparent advantage of this approach according to~\cite{Seg2018}, Section~3, Convergence rates and computational cost comparison, is that, in the case of $\mu$ and $\nu$ being discrete, the stochastic gradient descent method converges at a rate of $\mathcal{O}\left( n^{-\frac{1}{2}} \right)$, where $n$ is the iteration number, but only requires a cost of $\mathcal{O}(p^2)$ per iteration. In the case of both measures being continuous, the cost per iteration remains $\mathcal{O}(p^2)$, but convergence can only be ensured up to a stationary point.

To incorporate the estimated transport plan into a transport map using\ \ref{RelatRegSolOrigSolMap},~\cite{Seg2018} provide a method of approximating the barycentric projection, making use of a neural network once more, since this allows for the target map to be defined on all of $\Omega_1$. Making use of $F_\varepsilon$ as defined above and a map $f_\theta$ parameterized as a deep neural network with the parameters $\theta$ the following algorithm can be stated.

\begin{algorithm}\label{OTMapEstAlg}
	\caption{Transport Map Estimation; adapted from~\cite{Seg2018}, Algorithm~2}
	\KwResult{Estimate $T_{\theta}$ of the barycentric projection $\bar{\gamma}^\varepsilon$}
	\KwIn{$\mu \in \PM{\Omega_1}, \nu \in \PM{\Omega_2}, \varepsilon > 0$, cost function $c$, convex cost function $d$, batch size $p$, learning rate $\rho$}
	Initialize $T_\theta$\;
	Compute estimates of optimal dual variables $\varphi, \psi$ using Algorithm~\ref{OTPlanEstAlg} with $\mu, \nu, \varepsilon, c, p$ and $\rho$\;
	\While{$\text{\normalfont{}not converged}$}{
		sample a batch $(x_1, \dots, x_p)$ from $\mu$\;
		sample a batch $(y_1, \dots, y_p)$ from $\nu$\;
		$\theta := \theta - \rho \sum\limits_{i, j = 1}^p F_\varepsilon\big( \varphi(x_i), \psi(y_j) \big) \nabla_\theta\,d\big( y_j, T_\theta(x_i) \big)$\;
	}
\end{algorithm}

This algorithm may also be used to compute the opposite barycentric projection $g$ with respect to a convex cost function $d$ on $\Omega_1 \times \Omega_1$. The last term in line~6 of Algorithm~\ref{OTMapEstAlg} should then be $d\big( g(y_j), x_i \big)$. This is due to the symmetry of the optimal transport problem, as~\cite{Seg2018} points out.